# -*- coding: utf-8 -*-
"""assign4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZtOKTG_jLwrQruDrkW8PSbK3XXyRJv99
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Colab Notebooks/NLP_A4

import nltk
import re
from nltk.tokenize import word_tokenize
from nltk.stem.snowball import SnowballStemmer
from nltk.corpus import stopwords

!pip install datasets

"""## Data Preprocessing"""

from datasets import load_dataset
train_sst = load_dataset('sst', split='train', cache_dir='./data')
test_sst = load_dataset('sst', split='test', cache_dir='./data')

print(train_sst[0])
print(len(train_sst))
print(test_sst[0])
print(len(test_sst))

nltk.download('punkt')
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
stemmer = SnowballStemmer('english')

def preprocess_sentence(sentence):
    # remove punctuations and extra spaces
    sentence = re.sub(r'[^\w\s]', '', sentence)
    sentence = re.sub(r'\s+', ' ', sentence)
    # tokenize and stem
    tokens = word_tokenize(sentence)
    stemmed_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]
    
    return ' '.join(stemmed_tokens)

train_data = []
test_data = []
for example in train_sst:
    sentence = example['sentence']
    label = example['label']
    train_data.append((preprocess_sentence(sentence), label))

for example in test_sst:
    sentence = example['sentence']
    label = example['label']
    test_data.append((preprocess_sentence(sentence), label))

print(train_data[0])
print(len(train_data))
print(test_data[0])
print(len(test_data))

vocab = {}
for sentence, _ in train_data + test_data:
    for token in sentence.split():
        if token not in vocab:
            vocab[token] = len(vocab)
print(len(vocab))

"""## Elmo Model"""

import torch
import torch.nn as nn
import torch.optim as optim
import gensim.downloader as api
import numpy as np
from sklearn.metrics import classification_report

# Load pre-trained word2vec model
word2vec_model = api.load('word2vec-google-news-300')

# Define the ELMo model
class ELMo(nn.Module):
    def __init__(self, num_layers, hidden_size, dropout):
        super(ELMo, self).__init__()
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.dropout = nn.Dropout(p=dropout)
        self.lstm = nn.LSTM(input_size=300, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, batch_first=True)
        self.gamma = nn.Parameter(torch.ones((1,)))
        self.weights = nn.Parameter(torch.zeros((num_layers * 2,)))

    def forward(self, inputs):
        embeds = []
        # print("inputs size=",inputs.size())
        for i in range(inputs.size(1)):
            word = inputs[:, i, :]
            # print("word= ",word.shape)
            word = word.long().view(-1).tolist()
            word_embed = torch.from_numpy(word2vec_model[word]).unsqueeze(1).to(inputs.device)
            # print("word_embed= ",word_embed.shape)
            embeds.append(word_embed)
        # print("embeds = ", len(embeds), embeds[0].shape)
        embeds = torch.cat(embeds, dim=1)
        # print("embeds1 = ", embeds.shape)
        lstm_out, _ = self.lstm(embeds)
        # print("lstm_out = ", lstm_out.shape)
        embeddings = lstm_out[:, -1, :]
        # print("embeddings = ", embeddings.shape)
        for i in range(self.num_layers):
            alpha = nn.functional.softmax(self.weights[i:i+2], dim=0)
            embeddings = self.dropout(embeddings)
            embeddings = alpha[0] * embeddings + alpha[1] * lstm_out[:, -1, :]
        embeddings = self.gamma * embeddings
        return embeddings

# Define model hyperparameters
num_layers = 2
hidden_size = 256
dropout = 0.5
learning_rate = 0.001
batch_size = 32
num_epochs = 50

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
elmo = ELMo(num_layers=num_layers, hidden_size=hidden_size, dropout=dropout).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(elmo.parameters(), lr=learning_rate)

# Train the ELMo model
for epoch in range(num_epochs):
    epoch_loss = 0
    for i in range(0, len(train_data), batch_size):
        batch_data = train_data[i:i+batch_size]
        sentences = [data[0] for data in batch_data]
        labels = [data[1] for data in batch_data]
        sentence_lengths = [len(sentence.split()) for sentence in sentences]
        max_length = max(sentence_lengths)
        padded_sentences = np.zeros((len(sentences), max_length), dtype=np.int)
        for j, sentence in enumerate(sentences):
            token_ids = [vocab[token] for token in sentence.split() if token in vocab]
            padded_sentences[j, :len(token_ids)] = token_ids
        # inputs = torch.from_numpy(padded_sentences).to(device)
        # print("padded_sentences= ",padded_sentences.shape)
        inputs = torch.from_numpy(padded_sentences).unsqueeze(2).float().to(device)
        targets = torch.tensor(labels).to(device)
        optimizer.zero_grad()
        # print("input shape=",inputs.shape,targets.shape)
        outputs = elmo(inputs)
        targets = (targets > 0.5).long()
        # print(outputs.shape)
        # print(outputs)
        # print(targets)
        loss = criterion(outputs, targets)
        epoch_loss += loss.item()
        loss.backward()
        optimizer.step()
        # break
    print('Epoch: {}, Loss: {}'.format(epoch+1, epoch_loss))

# Save the pre-trained ELMo model
torch.save(elmo.state_dict(), "elmo_pretrained.pt")

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Generate embeddings for the entire dataset and save them
with torch.no_grad():
    embeddings = []
    for sentence, _ in train_data + test_data:
        tokens = sentence.split()
        token_ids = [vocab[token] for token in tokens]
        token_ids_tensor = torch.tensor(token_ids).unsqueeze(0).to(device)
        token_ids_tensor = torch.tensor(token_ids_tensor).unsqueeze(0).to(device)
        embedding = elmo(token_ids_tensor).squeeze(0).cpu().numpy()
        embeddings.append(embedding)
    print(len(embeddings),embeddings[0].shape)
    embeddings = np.vstack(embeddings)
    np.save("sst_embeddings.npy", embeddings)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve, auc


def plot_confusion_matrix(y_true, y_pred, labels):
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, ax=ax)
    ax.set_xlabel('Predicted Label')
    ax.set_ylabel('True Label')
    ax.set_xticklabels(labels)
    ax.set_yticklabels(labels)
    plt.show()



def plot_roc_curve(y_true, y_pred, label):
    fpr, tpr, _ = roc_curve(y_true, y_pred)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange',
             lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.show()

# y_true = [0, 1, 0, 1, 1, 0, 0, 1]
# y_pred = [0.1, 0.8, 0.2, 0.7, 0.6, 0.3, 0.4, 0.9]

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import classification_report
from tqdm import tqdm
from torch.utils.data import DataLoader, Dataset

# Define classifier

class Classifier(nn.Module):
    def __init__(self, input_size, output_size):
        super(Classifier, self).__init__()
        self.fc = nn.Linear(input_size, output_size)

    def forward(self, x):
        out = self.fc(x)
        return out

# Define hyperparameters
learning_rate = 0.001
num_epochs = 10
batch_size = 32

# Create classifier
classifier = Classifier(input_size=512, output_size=2).to(device)

# Define optimizer and loss function
optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss()


train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)

# Train classifier
for epoch in range(num_epochs):
    epoch_loss = 0
    for batch in tqdm(train_loader):
        inputs = inputs.to(device)
        targets = targets.to(device)
        sentences, labels = batch
        # print(inputs)
        # print(labels)
        # inputs = inputs.to(device)
        # labels = labels.to(device)
        sentence_lengths = [len(sentence.split()) for sentence in sentences]
        max_length = max(sentence_lengths)
        padded_sentences = np.zeros((len(sentences), max_length), dtype=np.int)
        for j, sentence in enumerate(sentences):
            token_ids = [vocab[token] for token in sentence.split() if token in vocab]
            padded_sentences[j, :len(token_ids)] = token_ids
        # inputs = torch.from_numpy(padded_sentences).to(device)
        inputs = torch.from_numpy(padded_sentences).unsqueeze(2).float().to(device)
        targets = torch.tensor(labels).to(device)
        targets = (targets > 0.5).long()
        # print(inputs.shape)
        embeddings = elmo(inputs)
        # print(embeddings.shape)
        # embeddings = embeddings.mean(dim=1)
        # print(embeddings.shape)
        outputs = classifier(embeddings)
        # print(outputs)
        # Forward pass
        _, predicted = torch.max(outputs.data, 1)
        # print(predicted)
        # Compute loss
        loss = criterion(outputs, targets)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}")

torch.save(classifier,"sst_classifier.pt")

# Define evaluation function
def evaluate(model, classifier, dataloader):
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        for batch in tqdm(dataloader):
            sentences, labels = batch
            # print(inputs)
            # print(labels)
            # inputs = inputs.to(device)
            # labels = labels.to(device)
            sentence_lengths = [len(sentence.split()) for sentence in sentences]
            max_length = max(sentence_lengths)
            padded_sentences = np.zeros((len(sentences), max_length), dtype=np.int)
            for j, sentence in enumerate(sentences):
                token_ids = [vocab[token] for token in sentence.split() if token in vocab]
                padded_sentences[j, :len(token_ids)] = token_ids
            # inputs = torch.from_numpy(padded_sentences).to(device)
            inputs = torch.from_numpy(padded_sentences).unsqueeze(2).float().to(device)
            targets = torch.tensor(labels).to(device)
            targets = (targets > 0.5).long()
            # print(inputs.shape)
            embeddings = model(inputs)
            # print(embeddings.shape)
            # embeddings = embeddings.mean(dim=1)
            # print(embeddings.shape)
            outputs = classifier(embeddings)
            # print(outputs)
            _, predicted = torch.max(outputs.data, 1)
            # print(predicted)
            y_true.extend(targets.tolist())
            y_pred.extend(predicted.tolist())
            
    # print(y_true)
    # print(y_pred)
    # y_true[0]=1
    # y_pred[0]=1
    report = classification_report(y_true, y_pred, digits=4)
    print("REPORT: ")
    print(report)
    # labels = ['Negative', 'Positive']
    # plot_confusion_matrix(y_true, y_pred, labels)
    label = 'Positive'
    plot_roc_curve(y_true, y_pred, label)

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load pre-trained ELMo model
model_path = 'elmo_pretrained.pt'
model = ELMo(2,256, 0.5).to(device)
model.load_state_dict(torch.load(model_path))

# Load test dataset
test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False)

# Define classifier
classifier = torch.load("sst_classifier.pt", map_location=device)

# Evaluate the model on the test set
evaluate(model,classifier,  test_dataloader)

